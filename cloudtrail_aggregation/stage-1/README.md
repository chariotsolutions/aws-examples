# Stage 1: Daily Aggregation

This stage takes a day's worth of files produced by CloudTrail, and writes them as
one or more NDJSON (newline-delimited JSON) files in a different destination.

The core challenge of this step is to identify the files belonging to a given day.
This requires recursively calling the `ListObjectsV2` API, to retrieve the list of
accounts and regions within those accounts.


## Prerequisites

You must have a bucket containing raw CloudTrail log files. This will normally be
created at the time you create your trail. If it's in a different account from the
one running this Lambda, it must have a bucket policy that allows `s3:ListBucket`
and `s3:GetObject` for the invoking account.

You must also have a bucket that will receive the aggregated files. If you create
this bucket in a different account, it must have a bucket policy that allows
s3:PutObject` for the invoking account.


## CloudFormation

You can deploy this Lambda using [CloudFormation](cloudformation.yml). This template
creates the following resources:

* The SQS queue used to trigger the Lambda, with an associated dead-letter queue.

* A Lambda to perform the transform, along with its execution role, log group, and
  trigger. Note that the Lambda has dummy source code; you must explicitly update
  with the contents of [lambda.py](lambda.py).

* An EventBridge Scheduler rule (along with its role) that writes a message to the
  SQS queue at 1 AM UTC.

For convenience, the script outputs the URL of the SQS queue.


## Triggering the Lambda

As deployed, the Lambda will be triggered by EventBridge at 1 AM UTC every day.

You can also use the Console to push a JSON message like the this:

```
{
  "month": 2,
  "day": 8,
  "year": 2024
}
```

To create a backlog of days to process, use the [populate_queue.py](populate_queue.py)
script, changing the `QUEUE_URL` constant to the actual trigger queue.


## Creating an Athena table

The file [table.ddl](table.ddl) contains DDL for the daily aggregation table. Edit
this file, replacing `MY_BUCKET` with the name of your destination bucket, then paste
into an Athena query window (you may also replace `cloudtrail_daily` with your prefered
destination prefix).

This files is based on the [DDL generated by the CloudTrail Console](https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html#create-cloudtrail-table-ct),
with the following changes:

* Add partition projection for the `year/month/day` format of the aggregated data.
  This projection covers the years 2013 (when CloudTrail was introduced) to 2038
  (the end of the Unix epoch). I don't believe that the wide range of years will
  affect performance, but feel free to cut it back to the range of your data.

* Use the default (`TEXTFILE`) input/output formats, rather than
  `com.amazon.emr.cloudtrail.CloudTrailInputFormat`. The latter format expects the
  files to be structured with a top-level `Records` element, which is not present
  in the files generated by my Lambda. It also appears to transform deeply nested
  sub-object (such as `requestParameters`) into strings.

* Use `org.openx.data.jsonserde.JsonSerDe` and ignore malformed JSON. This appears
  to stringify the same deeply nested objects (although it's not
  [documented](https://github.com/rcongiu/Hive-JSON-Serde) as doing so). The original
  DDL useds `org.apache.hive.hcatalog.data.JsonSerDe`, which cannot handle the deeply
  nested fields, and this failure also caused other fields to be nullified.

I have noticed a small number of unparseable rows (those where all fields are null)
using this configuration. I have not investigated what causes this, but suspect that
the definition for one of the "struct" fields is not up-to-date.
